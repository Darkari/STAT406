---
title: "STAT406 - Lecture 15 notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=6, message=FALSE, warning=FALSE)
```

## LICENSE
These notes are released under the 
"Creative Commons Attribution-ShareAlike 4.0 International" license. 
See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)
and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). 

## Lecture slides

Preliminary lecture slides are [here](STAT406-18-lecture-15-preliminary.pdf).


#### Instability of trees

Just like in the regression case, classification 
trees can be highly unstable (specifically: relatively small 
changes in the training set may result in 
comparably large changes in the corresponding tree). 

We illustrate the problem on the very simple graduate
school admissions example 
(3-class 2-dimensional covariates) we used in class. First
read the data:
```{r inst1, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
mm <- read.table('../Lecture14/T11-6.DAT', header=FALSE)
```
We transform the response variable `V3` into a factor:
```{r inst11}
mm$V3 <- as.factor(mm$V3)
```
To obtain better scaled plots, we re-scale one of the features
(so both explanatory variables have similar scales):
```{r inst12}
mm[,2] <- mm[,2] / 150
```
Now we train a classification tree on these data, using 
deviance-based (`information`) splits:
```{r inst13}
library(rpart)
a.t <- rpart(V3~V1+V2, data=mm, method='class', parms=list(split='information'))
```
To show how the tree changes when the data are perturbed
slightly, we create a new training set (`mm2`) that is equal to 
the original one, but change the responses of two observations:
```{r inst20}
mm2 <- mm
mm2[1,3] <- 2
mm2[7,3] <- 2
```
The following plot contains the new training set, with the two changed
observations:
```{r inst2, fig.width=6, fig.height=6, message=FALSE, warning=FALSE, fig.show='hold'}
plot(mm2[,1:2], pch=19, cex=1.5, col=c("red", "blue", "green")[mm2[,3]],
     xlab='GPA', 'GMAT', xlim=c(2,5), ylim=c(2,5))
points(mm[c(1,7),-3], pch='O', cex=1.1, col=c("red", "blue", "green")[mm[c(1,7),3]])
```

As above, we train a classification tree on the perturbed data:
```{r inst2.5, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
a2.t <- rpart(V3~V1+V2, data=mm2, method='class', parms=list(split='information'))
```
To visualize the differences between the two trees we build a fine grid of points
and compare the predicted probabilities of each class on each point on the grid. 
First, construct the grid:
```{r inst2.0}
aa <- seq(2, 5, length=200)
bb <- seq(2, 5, length=200)
dd <- expand.grid(aa, bb)
names(dd) <- names(mm)[1:2]
```
Now, compute the estimated conditional probabilities of each of the 3 classes
on each of the 40,000 points on the grid `dd`:
```{r inst2.1}
p.t <- predict(a.t, newdata=dd, type='prob')
p2.t <- predict(a2.t, newdata=dd, type='prob')
```
We now show the estimated probabilities of class "red" with each of the two
trees:
```{r inst2.2, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
filled.contour(aa, bb, matrix(p.t[,1], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
plot.axes={axis(1); axis(2)}, 
panel.last={points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]])})

filled.contour(aa, bb, matrix(p2.t[,1], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
plot.axes={axis(1); axis(2)},
panel.last={points(mm2[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm2[,3]]);
points(mm[c(1,7),-3], pch='O', cex=1.1, col=c("red", "blue", "green")[mm[c(1,7),3]])
})
```

Similarly, we do it for class "blue":
```{r kk}
# blues
filled.contour(aa, bb, matrix(p.t[,2], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
plot.axes={axis(1); axis(2)}, 
panel.last={ points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]])})

filled.contour(aa, bb, matrix(p2.t[,2], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
plot.axes={axis(1); axis(2)},
pane.last={points(mm2[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm2[,3]]);
points(mm[c(1,7),-3], pch='O', cex=1.1, col=c("red", "blue", "green")[mm[c(1,7),3]])
})
```

And finally, for class "green":
```{r kk2}
# greens
filled.contour(aa, bb, matrix(p.t[,3], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
plot.axes={axis(1); axis(2)}, panel.last={ points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]])})

filled.contour(aa, bb, matrix(p2.t[,3], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
plot.axes={axis(1); axis(2)},
pane.last={points(mm2[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm2[,3]]);
points(mm[c(1,7),-3], pch='O', cex=1.1, col=c("red", "blue", "green")[mm[c(1,7),3]])
})
```

We see that the regions that would be classified as "red" or "green", in particular,
change quite noticeably by only changing two points in the training set. 
Bagging can provide more stable classifiers and we discuss this strategy below.


## Bagging

Just as we did for regression, bagging consists of building an ensemble of
predictors (in this case, classifiers) using bootstrap samples. 
If we using *B* bootstrap samples, we will construct *B* classifiers, and
given a point **x**, we now have *B* estimated conditional probabilities 
for each of the possible *K* classes. 
Unlike what happens with regression problems, we now have a choice to make
when deciding how to combine the *B* outputs for each point. We can take
either: a majority vote over the *B* separate decisions, or we can 
average the *B* estimated probabilities for the *K* classes, to obtain
bagged estimated conditional probabilities. As discussed and illustrated
in class, the latter approach is usually preferred. 

To illustrate the increased stability of bagged classification trees, 
we repeat the experiment above: we build an ensemble of 1000 classification
trees trained on the original data, and a second ensemble (also of 1000
trees) using the slightly modified data. The first ensemble is
constructed as follows:
```{r bag0}
my.c <- rpart.control(minsplit=3, cp=1e-6, xval=10)
NB <- 1000
ts <- vector('list', NB)
set.seed(123)
n <- nrow(mm)
for(j in 1:NB) {
  ii <- sample(1:n, replace=TRUE)
  ts[[j]] <- rpart(V3~V1+V2, data=mm[ii,], method='class', parms=list(split='information'), control=my.c)
}
```
Note that the 1,000 trees are stored in the list `ts`. We do it again
with the slightly modified data as before, and store the trees in the ensemble
in the list `ts2`:
```{r bag02}
mm2 <- mm
mm2[1,3] <- 2
mm2[7,3] <- 2
NB <- 1000
ts2 <- vector('list', NB)
set.seed(123)
n <- nrow(mm)
for(j in 1:NB) {
  ii <- sample(1:n, replace=TRUE)
  ts2[[j]] <- rpart(V3~V1+V2, data=mm2[ii,], method='class', parms=list(split='information'), control=my.c)
}
```
We use the same fine grid as before to show the 
estimated conditional probabilities, this time
obtained with the two ensembles.
```{r grid0}
aa <- seq(2, 5, length=200)
bb <- seq(2, 5, length=200)
dd <- expand.grid(aa, bb)
names(dd) <- names(mm)[1:2]
```
To combine (average) the 1,000 estimated probabilities 
of each of the 3 classes for each of the 40,000 points 
in the grid `dd` I use the function `vapply`
and store the result in a 3-dimensional array. The
averaged probabilities over the 1000 bagged trees
can then obtained by averaging across the 3rd dimension. 
This approach may not be intuitively very clear at 
first sight. You are strongly encouraged to ignore it
and build these 40,000 bagged conditional probabilities
your own way instead. 
```{r bag1, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
pp0 <- vapply(ts, FUN=predict, FUN.VALUE=matrix(0, 200*200, 3), newdata=dd, type='prob')
pp <- apply(pp0, c(1, 2), mean)
pp02 <- vapply(ts2, FUN=predict, FUN.VALUE=matrix(0, 200*200, 3), newdata=dd, type='prob')
pp2 <- apply(pp02, c(1, 2), mean)
```
We now show the estimated conditional probabilities for "red"
in each point of the grid, with each of the two ensembles. Note 
how similar they are (and contrast this with the results
without bagging): 
```{r bag1.plot}
filled.contour(aa, bb, matrix(pp[,3], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
               plot.axes={axis(1); axis(2)},
                 panel.last={points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]])})
filled.contour(aa, bb, matrix(pp2[,3], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
               plot.axes={axis(1); axis(2)},
                 panel.last={points(mm2[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm2[,3]]);
                 points(mm[c(1,7),-3], pch='O', cex=1.2, col=c("red", "blue", "green")[mm[c(1,7),3]])
               })
```

You are strongly encouraged to obtain the corresponding plots comparing
the estimated conditional probabilities with both ensembles
for each of the other 2 classes.



<!-- # blues -->
<!-- filled.contour(aa, bb, matrix(pp[,2], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT', -->
<!--                plot.axes={axis(1); axis(2)},  -->
<!--                  panel.last={points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]])}) -->

<!-- # greens -->
<!-- filled.contour(aa, bb, matrix(pp[,3], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT', -->
<!--                plot.axes={axis(1); axis(2)},  -->
<!--                  panel.last={points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]])}) -->

<!-- pp2 <- apply(pp, 1, which.max) -->
<!-- pdf('gpa-bagg-pred-rpart.pdf') -->
<!-- image(aa, bb, matrix(as.numeric(pp2), 200, 200), col=c('pink', 'lightblue','lightgreen'), xlab='GPA', ylab='GMAT') -->
<!-- points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]]) -->
<!-- dev.off() -->

<!-- And with the modified data -->

<!-- mm2 <- mm -->
<!-- mm2[1,3] <- 2 -->
<!-- mm2[7,3] <- 2 -->

<!-- NB <- 1000 -->
<!-- ts <- vector('list', NB) -->
<!-- set.seed(123) -->
<!-- n <- nrow(mm) -->
<!-- for(j in 1:NB) { -->
<!--   ii <- sample(1:n, replace=TRUE) -->
<!--   ts[[j]] <- rpart(V3~V1+V2, data=mm2[ii,], method='class', parms=list(split='information'), control=my.c) -->
<!-- } -->

<!-- pp0 <- vapply(ts, FUN=predict, FUN.VALUE=matrix(0, 200*200, 3), newdata=dd, type='prob') -->
<!-- pp3 <- apply(pp0, c(1, 2), mean) -->

<!-- # reds -->
<!-- filled.contour(aa, bb, matrix(pp3[,1], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT', -->
<!--                plot.axes={axis(1); axis(2)}, -->
<!--                  panel.last={points(mm2[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm2[,3]]); -->
<!--                  points(mm[c(1,7),-3], pch='O', cex=1.2, col=c("red", "blue", "green")[mm[c(1,7),3]]) -->
<!--                }) -->

<!-- # blues -->
<!-- filled.contour(aa, bb, matrix(pp3[,2], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT', -->
<!--                plot.axes={axis(1); axis(2)}, -->
<!--                  panel.last={points(mm2[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm2[,3]]); -->
<!--                  points(mm[c(1,7),-3], pch='O', cex=1.2, col=c("red", "blue", "green")[mm[c(1,7),3]]) -->
<!--                }) -->

<!-- # greens -->
<!-- filled.contour(aa, bb, matrix(pp3[,3], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT', -->
<!--                plot.axes={axis(1); axis(2)}, -->
<!--                  panel.last={points(mm2[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm2[,3]]); -->
<!--                  points(mm[c(1,7),-3], pch='O', cex=1.2, col=c("red", "blue", "green")[mm[c(1,7),3]]) -->
<!--                }) -->

<!-- pp4 <- apply(pp3, 1, which.max) -->
<!-- pdf('gpa-bagg-pred2-rpart.pdf') -->
<!-- image(aa, bb, matrix(as.numeric(pp4), 200, 200), col=c('pink', 'lightblue','lightgreen'), xlab='GPA', ylab='GMAT') -->
<!-- points(mm2[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm2[,3]]) -->
<!-- points(mm[c(1,7),-3], pch='O', cex=1.2, col=c("red", "blue", "green")[mm[c(1,7),3]]) -->
<!-- dev.off() -->


# Random Forests

Even though using a *bagged* ensemble of trees helps to improve the stability of 
resulting predictor, it can be improved further. The main idea is to reduce the 
(conditional) potential correlation among bagged trees, as discussed in class. 
In `R` we use the funtion `randomForest` from the package with the same name.
The syntax is the same as that of `rpart`, but the tuning parameters 
for each of the *trees* in the *forest* are different from `rpart`. 
Refer to the help page if you need to modify them. 

```{r rf1, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
library(randomForest)
a.rf <- randomForest(V3~V1+V2, data=mm, ntree=500) 
```
Predictions can be obtained using the `predict` method, as usual, when 
you specify the `newdata` argument.
To visualize the Random Forest, we compute the corresponding
predicted conditional class probabilities on the 
relatively fine grid used before. The predicted 
conditional probabilities for class *red* are shown in the plot below
(how are these computed, exactly?)

```{r rf1.1, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
pp.rf <- predict(a.rf, newdata=dd, type='prob')
filled.contour(aa, bb, matrix(pp.rf[,1], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
               plot.axes={axis(1); axis(2)},
                 panel.last={points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]])
               })

```

And the predicted conditional probabilities for the rest of the classes are:

```{r rf2, fig.width=6, fig.height=6, message=FALSE, warning=FALSE, echo=FALSE}
filled.contour(aa, bb, matrix(pp.rf[,2], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
               plot.axes={axis(1); axis(2)},
                 panel.last={points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]])
               })
filled.contour(aa, bb, matrix(pp.rf[,3], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT',
               plot.axes={axis(1); axis(2)},
                 panel.last={points(mm[,-3], pch=19, cex=1.5, col=c("red", "blue", "green")[mm[,3]])
               })
```

A very interesting exercise would be to train a Random Forest on the perturbed data
and verify that the predicted conditional probabilities do not change much, 
as was the case for the bagged classifier. 

### Another example

We will now use a more interesting example. The ISOLET data, available 
here: 
[http://archive.ics.uci.edu/ml/datasets/ISOLET](http://archive.ics.uci.edu/ml/datasets/ISOLET), 
contains data 
on sound recordings of 150 speakers saying each letter of the
alphabet (twice). See the original source for more details. Since 
the full data set is rather large, here we only use a subset 
corresponding to the observations for the letters **C** and **Z**. 

We first load the training and test data sets, and force the response 
variable to be categorical, so that the `R` implementations of the
different predictors we will use below will build 
classifiers and not their regression counterparts:
```{r rf.isolet, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
xtr <- read.table('isolet-train-c-z.data', sep=',')
xte <- read.table('isolet-test-c-z.data', sep=',') 
xtr$V618 <- as.factor(xtr$V618)
xte$V618 <- as.factor(xte$V618)
```
We first train a Random Forest, using all the default parameters, and check 
its performance on the test set:
```{r rf.isolet2, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
library(randomForest)
set.seed(123)
a.rf <- randomForest(V618 ~ ., data=xtr, ntree=500) 
p.rf <- predict(a.rf, newdata=xte, type='response')
table(p.rf, xte$V618)
```
Note that the Random Forest only makes one mistake out of 120 observations
in the test set. The OOB error rate estimate is slightly over 2%, 
and we see that 500 trees is a reasonable forest size, in the sense
thate the OOB error rate estimate is stable. 

```{r rf.oob, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
a.rf
plot(a.rf, lwd=3, lty=1)
```
