{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT406 - Lecture 1 notes\n",
    "================\n",
    "Matias Salibian-Barrera\n",
    "2018-08-19\n",
    "\n",
    "#### LICENSE\n",
    "\n",
    "These notes are released under the \"Creative Commons Attribution-ShareAlike 4.0 International\" license. See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/) and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Lecture slides\n",
    "--------------\n",
    "\n",
    "The lecture slides will be here.\n",
    "\n",
    "Tools you may find useful during the course\n",
    "-------------------------------------------\n",
    "\n",
    "-   [R](http://www.cran.r-project.org/): This is the software we will use in the course. I will assume that you are familiar with it (in particular, that you know how to write **your own functions** and **loops**). If needed, there are plenty of resources on line to learn R.\n",
    "-   [RStudio](https://www.rstudio.com/products/RStudio/): The IDE (integrated development environment) of choice for R. Not necessary, but helpful.\n",
    "-   [Jupyter Notebooks](https://www.datacamp.com/community/blog/jupyter-notebook-r): With these you will be able to reproduce (and play with) all the examples I use in class. Not necessary, but helpful.\n",
    "\n",
    "Predictions using a linear model\n",
    "--------------------------------\n",
    "\n",
    "In this document we will explore (rather superficially) some challenges found when trying to estimate the forecasting properties (e.g. the mean squared prediction error) of a (linear) predictor. We will use the air-pollution data set, which I have split into a *training set* and a *test set*. The test set will be ignored when **training** our model (in the case of a linear model, \"**training**\" simply means \"**when estimating the vector of linear regression parameters**\").\n",
    "\n",
    "If you are interested in how these sets (*training* and *test*) were constructed: I ran the following script (you do not need to do this, as I am providing both data sets to you, but you can re-create them yourself if you want to). In the code chunk below, `x.tr` denotes the training set and `x.te` the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- read.csv('rutgers-lib-30861_CSV-1.csv')\n",
    "set.seed(123)\n",
    "ii <- sample(rep(1:4, each=15))\n",
    "# this is the training set `pollution-train.dat`\n",
    "x.tr <- x[ii != 2, ]\n",
    "# this is the test set `pollution-test.dat`\n",
    "x.te <- x[ii == 2, ]\n",
    "# then I saved them to disk:\n",
    "# write.csv(x.tr, file='pollution-train.dat', row.names=FALSE, quote=FALSE)\n",
    "# write.csv(x.te, file='pollution-test.dat', row.names=FALSE, quote=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the above, we will now read the **training** data set from the file `pollution-train.dat`, which is available [here](pollution-train.dat), and check that it was read properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.tr <- read.table('pollution-train.dat', header=TRUE, sep=',')\n",
    "# sanity check\n",
    "head(x.tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable is `MORT`. Our first step is to fit a linear regression model with all available predictors and look at a few diagnostic plots where everything looks fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full <- lm(MORT ~ . , data=x.tr)\n",
    "plot(full, which=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(full, which=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also take a look at the estimated coeficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit appears to be routine, and reasonable (why? what did I check to come to this conclusion?).\n",
    "\n",
    "### A new focus: prediction\n",
    "\n",
    "This course will be primarily concerned with making (good) predictions for cases (data points) that we may have not observed yet (future predictions). This is a bit different from the focus of other Statistics courses you may have taken. You will see later in the course that what you learned in other Statistics courses (e.g. trade-offs between flexibility and stability of different models, uncertainty and standard techniques to reduce it, etc.) will prove to be critical for building good predictions.\n",
    "\n",
    "As a simple example, in the rest of this note we will compare the quality of this model's predictions with those of a simpler (smaller) linear model with only 5 predictors. For this illustrative example, we will not worry about how these 5 explanatory variables were selected, however, this will play a **critical** role later in the course).\n",
    "\n",
    "We now fit this **reduced** model and look at the estimated parameters and diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data=x.tr)\n",
    "summary(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(reduced, which=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(reduced, which=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the reduced linear model (with 5 predictors) does not seem to provide a fit as good as the one we get with full model, it is still acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum( resid(reduced)^2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum( resid(full)^2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This observation should be obvious to you, since, as you already now, a model will **always** yield a better fit to the data in terms of residual sum of squares than any of its submodels (i.e. any model using a subset of the explanatory variables). I expect you to be able to formally prove the last satement.\n",
    "\n",
    "Our question of interest here is: \"Which model produces better predictions?\" In many cases one is interested in predicting future observations, i.e. predicting the response variable for data that was not available when the model / predictor was *fit* or *trained*. As we discussed in class, a reasonably fair comparison can be obtined by comparing the mean squared predictions of these two linear models on the test set, which we read into `R` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.te <- read.table('pollution-test.dat', header=TRUE, sep=',')\n",
    "head(x.te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the predicted values for the test set with both the **full** and **reduced** models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.te$pr.full <- predict(full, newdata=x.te)  \n",
    "x.te$pr.reduced <- predict(reduced, newdata=x.te)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute the corresponding mean squared prediction errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(x.te, mean( (MORT - pr.full)^2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(x.te, mean( (MORT - pr.reduced)^2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the reduced model (that did not fit the data as well as the full model) nevertheless produced better predictions (smaller mean squared prediction errors) on the test set.\n",
    "\n",
    "At this point you should put on your critical / skeptical hat and wonder if this did not happen *by chance*, i.e. if this may be just an artifact of the specific training/test partition we used. The following simple experiment shows that this is not the case. It would be a **very good exercise** for you to repeat it many times (100, say) to verify my claim.\n",
    "\n",
    "First, read the whole data and create a new training / test random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with different partitions\n",
    "x <- read.csv('rutgers-lib-30861_CSV-1.csv')\n",
    "set.seed(456)\n",
    "ii <- sample(rep(1:4, each=15))\n",
    "x.tr <- x[ii != 2, ]\n",
    "x.te <- x[ii == 2, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code chunk, I used `x.tr` to denote the training set and `x.te` for the test set. Now, fit the full and reduced models on this new training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full <- lm(MORT ~ . , data=x.tr)\n",
    "reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data=x.tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, estimate the mean squared prediction error of these models with their squared prediction error on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.te$pr.full <- predict(full, newdata=x.te)\n",
    "x.te$pr.reduced <- predict(reduced, newdata=x.te)\n",
    "with(x.te, mean( (MORT - pr.full)^2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(x.te, mean( (MORT - pr.reduced)^2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the estimated mean squared prediction error of the reduced model is again considerably smaller than that of the full model (even though the latter always fits the training set better than the reduced one)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
