---
title: "STAT406 - Lecture 18 notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=6, 
                      message=FALSE, warning=FALSE, cache=TRUE) #, eval = FALSE)
```

## LICENSE
These notes are released under the 
"Creative Commons Attribution-ShareAlike 4.0 International" license. 
See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)
and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). 

## Lecture slides

Preliminary lecture slides will be here.

## What is Adaboost doing, *really*? 

Following the work of Friedman, Hastie, and Tibshirani
[here](https://doi.org/10.1214/aos/1016218223) (see also
Chapter 10 of [HTF09]), we saw in class that Adaboost can be 
interpreted as fitting an *additive model* in a stepwise (greedy) way,
using an exponential loss. 
It is then easy to prove that Adaboost.M1 
is computing an approximation to the *optimal classifier*
G( x ) = log[ P( Y = 1 | X = x ) / P( Y = -1 | X = x ) ] / 2,
where *optimal* here is taken with respect to the **exponential loss** 
function. More specifically, Adaboost.M1 is using an 
additive model to approximate that function. In other words, Boosting is
attempting to find functions $f_1$, $f_2$, ..., $f_N$ such that 
$G(x) = \sum_i f_i( x^{(i)} )$, where $x^{(i)}$ is a sub-vector
of $x$ (i.e. the function $f_i$ only depends on *some* of the
available features, typically a few of them: 1 or 2, say). Note
that each $f_i$ generally depends on a different subset of 
features than the other $f_j$'s. 

Knowing the function the boosting algorithm is approximating (even
if it does it in a greedy and suboptimal way), allows us to 
understand when the algorithm is expected to work well,
and also when it may not work well. 
In particular, it provides one way to choose the complexity of the 
*weak lerners* used to construct the ensemble. For an example
you can refer to the corresponding lab activity. 

### A more challenging example, the `email spam` data

The email spam data set is a relatively classic data set 
containing 57 features (potentially explanatory variables) 
measured on 4601 email messages. The goal is to predict
whether an email is *spam* or not. The 57 features are 
a mix of continuous and discrete variables. More information
can be found at
[https://archive.ics.uci.edu/ml/datasets/spambase](https://archive.ics.uci.edu/ml/datasets/spambase).

We first load the data and randomly separate it into a training and
a test set. A more thorough analysis would be to use 
*full* K-fold cross-validation, but given the computational
complexity, I decided to leave the rest of this 
3-fold CV exercise to the reader. 
```{r spam.1, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
data(spam, package='ElemStatLearn')
n <- nrow(spam)
set.seed(987)
ii <- sample(n, floor(n/3))
spam.te <- spam[ii, ]
spam.tr <- spam[-ii, ]
```
We now use Adaboost with 500 iterations, using *stumps* (1-split
trees) as our
weak learners / classifiers, and check the performance on
the test set:
```{r spam.2, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
library(adabag)
onesplit <- rpart.control(cp=-1, maxdepth=1, minsplit=0, xval=0)
bo1 <- boosting(spam ~ ., data=spam.tr, boos=FALSE, mfinal=500, control=onesplit)
pr1 <- predict(bo1, newdata=spam.te)
table(spam.te$spam, pr1$class) # (pr1$confusion)
```
The classification error rate on the test set is rather high (`r pr1$error`). We now
compare it with that of a Random Forest:

```{r spam.3, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
library(randomForest)
set.seed(123) 
a <- randomForest(spam ~ . , data=spam.tr) # , ntree=500)
```
We look at the output
```{r spam.3.rf, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
a
```
Note that the OOB estimate of the classification error rate 
is `r a$err.rate[a$ntree, "OOB"]`. 
The number of trees used seems to be appropriate in terms
of the stability of the OOB error rate estimate:
```{r spam.plot.rf, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
plot(a)
```

Now use the test set to estimate the error rate of the Random Forest 
(for a fair comparison with the one computed with boosting) and obtain
```{r spam.error.rate.rf, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
pr.rf <- predict(a, newdata=spam.te, type='response')
table(spam.te$spam, pr.rf)
```
The performance of Random Forests on this test set is better than that of 
boosting (recall that the estimated classification error rate 
for 1-split trees-based Adaboost was 
`r pr1$error`, while for the Random Forest is `r mean( spam.te$spam != pr.rf )` 
on the test set and `r a$err.rate[a$ntree, "OOB"]` using OOB). 

Is there *any room for improvement* for Adaboost? 
As we discussed in class, depending on the interactions that may be 
present in the *true classification function*, we might be able to 
improve our boosting classifier by slightly increasing the complexity
of our base ensemble members. Here we try to use 3-split classification
trees, instead of the 1-split ones used above:
```{r spam.4, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
threesplits <- rpart.control(cp=-1, maxdepth=3, minsplit=0, xval=0)
bo3 <- boosting(spam ~ ., data=spam.tr, boos=FALSE, mfinal=500, control=threesplits)
pr3 <- predict(bo3, newdata=spam.te)
(pr3$confusion)
```
The number of element on the boosting ensemble appears to be
appropriate:
```{r spam.5, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
plot(errorevol(bo3, newdata=spam.te))
```

There is, in fact, a noticeable improvement in performance on this
test set. The estimated classification error rate of AdaBoost using
3-split trees on this test set is
`r pr3$error`. Recall that the estimated classification error rate
for the Random Forest was `r mean( spam.te$spam != pr.rf )`
(or `r a$err.rate[a$ntree, "OOB"]` using OOB). 

As mentioned above you are strongly encouraged to finish this analysis
by doing a complete K-fold CV analysis in order to compare boosting with random 
forests on these data. 

## Gradient boosting

Discussed in class.

## Neural Networks

Discussed in class.

### An example with a simple neural network
This example using the ISOLET data illustrates the use of simple
neural networks (NNs), and also highlights some issues of which it may 
be important to be aware. As we discussed in class, NNs typically have 
more parameters than observations and a number of tuning parameters
that need to be chosen by the user. Among these: the number of 
hidden layers, the number of units in each layer, the *activation function*,
the *loss function*, a decaying factor, and the initial point 
at which to start the optimization iterations. In the example below we illustrate 
some difficulties that can be encountered when trying to find 
which tuning parameters to use to train a NN.

In order to focus on the concepts behind NN, we will use the `nnet` 
package in `R`. This package is a very simple implementation 
of NNs with a single hidden layer, and relies on standard optimization
algorithms to train it. Such simple setting will allow us to 
separate implementation / optimization issues from the underlying
model and ideas behind NN, which carry over naturally to more
complex NNs. 

For our example we will use again the ISOLET data which is available here: [http://archive.ics.uci.edu/ml/datasets/ISOLET](http://archive.ics.uci.edu/ml/datasets/ISOLET), along with more information about it. It contains data on sound recordings of 150 speakers saying each letter of the alphabet (twice). See the original source for more details. The full data file is rather large and available in compressed form. 
Instead, we will read it from a private copy in plain text form I made 
available on Dropbox.  

#### "C" and "Z"
First we look at building a classifier to identify the letters C and Z. This 
is the simplest scenario and it will help us fix ideas. We now read the 
full data set, and extract the training and test rows corresponding to those
two letters:
```{r isolet0}
library(nnet)
xx.tr <- read.table('https://www.dropbox.com/s/b0avl1w6pcevfc5/isolet-train.data?dl=1', sep=',')
xx.te <- read.table('https://www.dropbox.com/s/lrdrj6u399to1h6/isolet-test.data?dl=1', sep=',')
lets <- c(3, 26)
LETTERS[lets]
# Training set
x.tr <- xx.tr[ xx.tr$V618 %in% lets, ]
x.tr$V618 <- as.factor(x.tr$V618)
# Test set
x.te <- xx.te[ xx.te$V618 %in% lets, ]
truth <- x.te$V618 <- as.factor(x.te$V618)
```
We train a NN with a single hidden layer, and a single unit in the hidden layer. 
```{r isolet1}
set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=1, decay=0, maxit=1500, MaxNWts=2000)
```
Note the slow convergence. The final value of the objective value was:
```{r isolet1.1} 
a1$value
```
The error rate on the training set ("goodness of fit") is
```{r isolet1.2}
b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)
```
We see that this NN fits the training set perfectly. Is this desirable? 

We now run the algorithm again, with a different starting point. 
```{r isolet2}
set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=1, decay=0, maxit=1500, MaxNWts=2000)
```
Compare
the attained value of the objective and the error rate on the training set
with those above (`r round(a1$value, 6)` and 0, respectively):
```{r isolet2.1}
a2$value
b2 <- predict(a2, type='class') #, type='raw')
mean(b2 != x.tr$V618)
```
So, we see that the second run of NN produces a much worse solution.
How are their performances on the test set?
```{r isolet2.2}
b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)
b2 <- predict(a2, newdata=x.te, type='class') #, type='raw')
mean(b2 != x.te$V618)
```
The second (worse) solution performs better on the test set. 

What if we add more units to the hidden layer? We increase the
number of units on the hidden layer from 3 to 6. 
```{r isolet3}
set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0, maxit=1500, MaxNWts=2000, trace=FALSE)
set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0, maxit=1500, MaxNWts=2000, trace=FALSE)
```
The objective functions are 
```{r isolet3.1}
a1$value
a2$value
```
respectively, and their performance on the training and test sets are:
```{r isolet4}
b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)
b2 <- predict(a2, type='class') #, type='raw')
mean(b2 != x.tr$V618)

b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)
b2 <- predict(a2, newdata=x.te, type='class') #, type='raw')
mean(b2 != x.te$V618)
```
Again we note that the (seemingly much) worse solution (in terms of the objective
function whose optimization defines the NN) performs better 
on the test set. 

What if we add a decaying factor as a form of regularization? 
```{r isolet10}
set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0.05, maxit=500, MaxNWts=2000, trace=FALSE)
a1$value
set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0.05, maxit=500, MaxNWts=2000, trace=FALSE)
a2$value
```
Now the two solutions starting from these random initial values 
are the same (the reader is encouraged to 
try more random starts). How does this NN do on the training and test sets?
```{r isolet10.1}
b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)
b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)
```
Note that this "regularized" solution which corresponds to a 
slightly better solution than the worse one above in terms
of objective function (but still much worse than the best ones)
performs noticeably better on the test set. This seem to suggest
that it is not easy to select which of the many local extrema to used
based  on the objective function values they attain. 

Another tuning parameter we can vary is the number of units
in the hidden layer, which will also increase significantly the
number of possible weight parameters in our model. 
The above solution uses `r length(a2$wts)` weights. We now add more 
units to the hidden layer (6 instead of 3) and increase the limit on
the number of allowable weights to 4000: 
```{r isolet11}
set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=6, decay=0.05, maxit=500, MaxNWts=4000, trace=FALSE)
a1$value
set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=6, decay=0.05, maxit=500, MaxNWts=4000, trace=FALSE)
a2$value
b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)

b2 <- predict(a2, type='class') #, type='raw')
mean(b2 != x.tr$V618)

b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)

b2 <- predict(a2, newdata=x.te, type='class') #, type='raw')
mean(b2 != x.te$V618)
```
Note that both of these two distinct solutions fit the training set 
well exactly (0 apparent error rate), and have the same performance
on the test set. We leave it to the reader to perform a more
exhaustive study of the prediction properties of these solutions
using an appropriate CV experiment. 

#### More letters

We now repeat the same exercise above but on a 4-class
setting. 

```{r iso20}
lets <- c(3, 7, 9, 26)
LETTERS[lets]

x.tr <- xx.tr[ xx.tr$V618 %in% lets, ]
x.tr$V618 <- as.factor(x.tr$V618)

# testing set
x.te <- xx.te[ xx.te$V618 %in% lets, ]
truth <- x.te$V618 <- as.factor(x.te$V618)


set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=1, decay=0, maxit=1500, MaxNWts=2000, trace=FALSE)
a1$value
length(a1$wts)
set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=1, decay=0, maxit=1500, MaxNWts=2000, trace=FALSE)
a2$value
length(a2$wts)
b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)

b2 <- predict(a2, type='class') #, type='raw')
mean(b2 != x.tr$V618)

b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)

b2 <- predict(a2, newdata=x.te, type='class') #, type='raw')
mean(b2 != x.te$V618)
```

```{r isolet22}
set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0, maxit=1500, MaxNWts=2000, trace=FALSE)

set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0, maxit=1500, MaxNWts=2000, trace=FALSE)

b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)

b2 <- predict(a2, type='class') #, type='raw')
mean(b2 != x.tr$V618)

b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)

b2 <- predict(a2, newdata=x.te, type='class') #, type='raw')
mean(b2 != x.te$V618)
```

```{r isolate30}
set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0.05, maxit=500, MaxNWts=2000, trace=FALSE)

set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0.05, maxit=500, MaxNWts=2000, trace=FALSE)

b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)

b2 <- predict(a2, type='class') #, type='raw')
mean(b2 != x.tr$V618)

b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)

b2 <- predict(a2, newdata=x.te, type='class') #, type='raw')
mean(b2 != x.te$V618)
```

```{r isolate31}
set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=6, decay=0.05, maxit=500, MaxNWts=4000, trace=FALSE)

set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=6, decay=0.05, maxit=500, MaxNWts=4000, trace=FALSE)

b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)

b2 <- predict(a2, type='class') #, type='raw')
mean(b2 != x.tr$V618)

b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)

b2 <- predict(a2, newdata=x.te, type='class') #, type='raw')
mean(b2 != x.te$V618)
```

#### Even more letters
```{r isolate40}
lets <- c(3, 5, 7, 9, 12, 13, 26)
LETTERS[lets]

x.tr <- xx.tr[ xx.tr$V618 %in% lets, ]
x.tr$V618 <- as.factor(x.tr$V618)

# testing set
x.te <- xx.te[ xx.te$V618 %in% lets, ]
truth <- x.te$V618 <- as.factor(x.te$V618)

set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=4, decay=0, maxit=1500, MaxNWts=3000, trace=FALSE)

set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=4, decay=0, maxit=1500, MaxNWts=3000, trace=FALSE)

b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)

b2 <- predict(a2, type='class') #, type='raw')
mean(b2 != x.tr$V618)

b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)

b2 <- predict(a2, newdata=x.te, type='class') #, type='raw')
mean(b2 != x.te$V618)
```

```{r isolate41}
set.seed(123)
a1 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0.3, maxit=1500, MaxNWts=2000, trace=FALSE)

set.seed(456)
a2 <- nnet(V618 ~ ., data=x.tr, size=3, decay=0.3, maxit=1500, MaxNWts=2000, trace=FALSE)

b1 <- predict(a1, type='class') #, type='raw')
mean(b1 != x.tr$V618)

b2 <- predict(a2, type='class') #, type='raw')
mean(b2 != x.tr$V618)

b1 <- predict(a1, newdata=x.te, type='class') #, type='raw')
mean(b1 != x.te$V618)

b2 <- predict(a2, newdata=x.te, type='class') #, type='raw')
mean(b2 != x.te$V618)
```



#### Additional resources for discussion (refer to the lecture for context)

* [https://arxiv.org/abs/1412.6572](https://arxiv.org/abs/1412.6572)
* [https://arxiv.org/abs/1312.6199](https://arxiv.org/abs/1312.6199)
* [https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html](https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html)
* [https://medium.com/intuitionmachine/the-deeply-suspicious-nature-of-backpropagation-9bed5e2b085e](https://medium.com/intuitionmachine/the-deeply-suspicious-nature-of-backpropagation-9bed5e2b085e)


