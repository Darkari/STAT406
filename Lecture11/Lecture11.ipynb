{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT406 - Lecture 11 notes\n",
    "================\n",
    "Matias Salibian-Barrera\n",
    "2018-10-10\n",
    "\n",
    "#### LICENSE\n",
    "\n",
    "These notes are released under the \"Creative Commons Attribution-ShareAlike 4.0 International\" license. See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/) and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Lecture slides\n",
    "--------------\n",
    "\n",
    "Preliminary lecture slides are [here](STAT406-18-lecture-11-preliminary.pdf).\n",
    "\n",
    "Pruning regression trees\n",
    "------------------------\n",
    "\n",
    "The stopping criteria generally used when fitting regression trees do not take into account explicitly the complexity of the tree. Hence, we may end up with an overfitting tree, which typically results in a decline in the quality of the corresponding predictions. As discussed in class, one solution is to purposedly grow / train a very large overfitting tree, and then prune it. One can also estimate the corresponding MSPE of each tree in the prunning sequence and choose an optimal one. The function `rpart` implements this approach, and we illustrate it below. We force `rpart` to build a very large tree via the arguments of the function `rpart.control`. At the same time, to obtain a good picture of the evolution of MSPE for different subtrees, we set the smallest complexity parameter to be considered by the cross-validation experiment to a very low value (here we use `1e-8`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "data(Boston, package='MASS')\n",
    "# split data into a training and\n",
    "# a test set\n",
    "set.seed(123456) \n",
    "n <- nrow(Boston)\n",
    "ii <- sample(n, floor(n/4))\n",
    "dat.te <- Boston[ ii, ]\n",
    "dat.tr <- Boston[ -ii, ]\n",
    "\n",
    "myc <- rpart.control(minsplit=3, cp=1e-8, xval=10)\n",
    "set.seed(123)\n",
    "bos.to <- rpart(medv ~ ., data=dat.tr, method='anova',\n",
    "                control=myc)\n",
    "plot(bos.to, compress=TRUE) # type='proportional')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the predictions of this large tree are not very good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions are poor, unsurprisingly\n",
    "pr.to <- predict(bos.to, newdata=dat.te, type='vector')\n",
    "with(dat.te, mean((medv - pr.to)^2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prune we explore the *CP table* returned in the `rpart` object to find the value of the complexity parameter with optimal estimated prediction error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printcp(bos.to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probably better and easier to find this optimal value *programatically* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( b <- bos.to$cptable[which.min(bos.to$cptable[,\"xerror\"]),\"CP\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the function `prune` on the `rpart` object setting the complexity parameter to the estimated optimal value found above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos.t3 <- prune(bos.to, cp=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the optimally pruned tree looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(bos.t3, uniform=FALSE, margin=0.01)\n",
    "text(bos.t3, pretty=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can verify that the predictions of the pruned tree on the test set are better than before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions are better\n",
    "pr.t3 <- predict(bos.t3, newdata=dat.te, type='vector')\n",
    "with(dat.te, mean((medv - pr.t3)^2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it would be a **very good exercise** for you to compare the MSPE of the pruned tree with that of several of the alternative methods we have seen in class so far, **without using a training / test split**.\n",
    "\n",
    "Note that pruning doesn't always improve a tree. For example, if we prune the first tree we fit in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we prune the original tree?\n",
    "set.seed(123)\n",
    "bos.t <- rpart(medv ~ ., data=dat.tr, method='anova')\n",
    "b <- bos.t$cptable[which.min(bos.t$cptable[,\"xerror\"]),\"CP\"]\n",
    "bos.t4 <- prune(bos.t, cp=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the same tree as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(bos.t4, uniform=FALSE, margin=0.01)\n",
    "text(bos.t4, pretty=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the original tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(bos.t, uniform=FALSE, margin=0.01)\n",
    "text(bos.t, pretty=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instability of regression trees\n",
    "-------------------------------\n",
    "\n",
    "Trees can be rather unstable, in the sense that small changes in the training data set may result in relatively large differences in the fitted trees. As a simple illustration we randomly split the `Boston` data used before into two halves and fit a regression tree to each portion. We then display both trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instability of trees...\n",
    "library(rpart)\n",
    "data(Boston, package='MASS')\n",
    "set.seed(654321)\n",
    "n <- nrow(Boston)\n",
    "ii <- sample(n, floor(n/2))\n",
    "dat.t1 <- Boston[ -ii, ]\n",
    "bos.t1 <- rpart(medv ~ ., data=dat.t1, method='anova')\n",
    "plot(bos.t1, uniform=FALSE, margin=0.01)\n",
    "text(bos.t1, pretty=TRUE, cex=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.t2 <- Boston[ ii, ]\n",
    "bos.t2 <- rpart(medv ~ ., data=dat.t2, method='anova')\n",
    "plot(bos.t2, uniform=FALSE, margin=0.01)\n",
    "text(bos.t2, pretty=TRUE, cex=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we would expect both random halves of the same (moderately large) training set to beat least qualitatively similar, Note that the two trees are rather different. To compare with a more stable predictor, we fit a linear regression model to each half, and look at the two sets of estimated coefficients side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bos.lmf <- lm(medv ~ ., data=Boston)\n",
    "bos.lm1 <- lm(medv ~ ., data=dat.t1)\n",
    "bos.lm2 <- lm(medv ~ ., data=dat.t2)\n",
    "cbind(round(coef(bos.lm1),2),\n",
    "round(coef(bos.lm2),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most of the estimated regression coefficients are similar, and all of them are at least qualitatively comparable.\n",
    "\n",
    "Bagging\n",
    "-------\n",
    "\n",
    "One strategy to obtain more stable predictors is called **Bootstrap AGGregatING** (bagging). It can be applied to many predictors (not only trees), and it generally results in larger improvements in prediction quality when it is used with predictors that are flexible (low bias), but highly variable.\n",
    "\n",
    "The justification and motivation were discussed in class. Intuitively we are averaging the predictions obtained from an estimate of the \"average prediction\" we would have computed had we had access to several (many?) independent training sets (samples).\n",
    "\n",
    "There are several (many?) `R` packages implementing bagging for different predictors, with varying degrees of flexibility (the implementations) and user-friendliness. However, for pedagogical and illustrative purposes, in these notes I will *bagg* by hand.\n",
    "\n",
    "<!-- ### Bagging by hand -->\n",
    "<!-- Again, to simplify the discussion and presentation, in order to evaluate  -->\n",
    "<!-- prediction quality I will split the  -->\n",
    "<!-- data (`Boston`) into a training and a test set. We do this now: -->\n",
    "<!-- ```{r bag1, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n",
    "<!-- set.seed(123456) -->\n",
    "<!-- n <- nrow(Boston) -->\n",
    "<!-- ii <- sample(n, floor(n/4)) -->\n",
    "<!-- dat.te <- Boston[ ii, ] -->\n",
    "<!-- dat.tr <- Boston[ -ii, ] -->\n",
    "<!-- ``` -->\n",
    "<!-- I will now train $N = 5$ trees and average their predictions.  -->\n",
    "<!-- Note that, in order to illustrate the process more -->\n",
    "<!-- clearly, I will compute and store the $N \\times n_e$ -->\n",
    "<!-- predictions, where $n_e$ denotes the number of observations in  -->\n",
    "<!-- the test set. This is not the best (most efficient) way of implementing *bagging*, -->\n",
    "<!-- but the main purpose here is to understand **what** we are doing. Also note that -->\n",
    "<!-- an alternative (better in terms of reusability of the -->\n",
    "<!-- ensamble, but maybe still not the most efficient option) would be -->\n",
    "<!-- to store the $N$ trees directly. This would also allow for -->\n",
    "<!-- more elegant and easy to read code. Once again, this approach  -->\n",
    "<!-- will be sacrificed in the altar of clarity of presentation and  -->\n",
    "<!-- pedagogy (but do try it yourself!) -->\n",
    "<!-- First create an array where we will store all the predictions: -->\n",
    "<!-- ```{r bag2, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n",
    "<!-- N <- 5 -->\n",
    "<!-- myps <- array(NA, dim=c(nrow(dat.te), N)) -->\n",
    "<!-- con <- rpart.control(minsplit=3, cp=1e-3, xval=1) -->\n",
    "<!-- ``` -->\n",
    "<!-- The last object (`con`) contains my options to train large -->\n",
    "<!-- (potentially overfitting) trees.  -->\n",
    "<!-- ```{r bag3, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n",
    "<!-- n.tr <- nrow(dat.tr) -->\n",
    "<!-- set.seed(123456) -->\n",
    "<!-- for(j in 1:N) { -->\n",
    "<!--   ii <- sample(n.tr, replace=TRUE) -->\n",
    "<!--   tmp <- rpart(medv ~ ., data=dat.tr[ii, ], method='anova', control=con) -->\n",
    "<!--   myps[,j] <- predict(tmp, newdata=dat.te, type='vector') -->\n",
    "<!-- } -->\n",
    "<!-- pr.bagg <- rowMeans(myps) -->\n",
    "<!-- with(dat.te, mean( (medv - pr.bagg)^2 ) ) -->\n",
    "<!-- ``` -->\n",
    "<!-- And compare with predictions from the pruned tree, and the -->\n",
    "<!-- ones from other predictors discussed in the previous note: -->\n",
    "<!-- ```{r bag4, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n",
    "<!-- myc <- rpart.control(minsplit=3, cp=1e-8, xval=10) -->\n",
    "<!-- set.seed(123) -->\n",
    "<!-- bos.to <- rpart(medv ~ ., data=dat.tr, method='anova', -->\n",
    "<!--                 control=myc) -->\n",
    "<!-- b <- bos.to$cptable[which.min(bos.to$cptable[,\"xerror\"]),\"CP\"] -->\n",
    "<!-- bos.t3 <- prune(bos.to, cp=b) -->\n",
    "<!-- pr.t3 <- predict(bos.t3, newdata=dat.te, type='vector') -->\n",
    "<!-- with(dat.te, mean((medv - pr.t3)^2) ) -->\n",
    "<!-- ``` -->\n",
    "<!-- What if we *bagg* $N = 10$ trees?  -->\n",
    "<!-- ```{r bag10, fig.width=5, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE} -->\n",
    "<!-- N <- 10 -->\n",
    "<!-- myps <- array(NA, dim=c(nrow(dat.te), N)) -->\n",
    "<!-- n.tr <- nrow(dat.tr) -->\n",
    "<!-- set.seed(123456) -->\n",
    "<!-- for(j in 1:N) { -->\n",
    "<!--   ii <- sample(n.tr, replace=TRUE) -->\n",
    "<!--   tmp <- rpart(medv ~ ., data=dat.tr[ii, ], method='anova', control=con) -->\n",
    "<!--   myps[,j] <- predict(tmp, newdata=dat.te, type='vector') -->\n",
    "<!-- } -->\n",
    "<!-- pr.bagg <- rowMeans(myps) -->\n",
    "<!-- with(dat.te, mean( (medv - pr.bagg)^2 ) ) -->\n",
    "<!-- ``` -->\n",
    "<!-- or $N = 100$ trees?  -->\n",
    "<!-- ```{r bag100, fig.width=5, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE} -->\n",
    "<!-- N <- 100 -->\n",
    "<!-- myps <- array(NA, dim=c(nrow(dat.te), N)) -->\n",
    "<!-- n.tr <- nrow(dat.tr) -->\n",
    "<!-- set.seed(123456) -->\n",
    "<!-- for(j in 1:N) { -->\n",
    "<!--   ii <- sample(n.tr, replace=TRUE) -->\n",
    "<!--   tmp <- rpart(medv ~ ., data=dat.tr[ii, ], method='anova', control=con) -->\n",
    "<!--   myps[,j] <- predict(tmp, newdata=dat.te, type='vector') -->\n",
    "<!-- } -->\n",
    "<!-- pr.bagg <- rowMeans(myps) -->\n",
    "<!-- with(dat.te, mean( (medv - pr.bagg)^2 ) ) -->\n",
    "<!-- ``` -->\n",
    "<!-- or $N = 1000$ trees?  -->\n",
    "<!-- ```{r bag1000, fig.width=5, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE} -->\n",
    "<!-- N <- 1000 -->\n",
    "<!-- myps <- array(NA, dim=c(nrow(dat.te), N)) -->\n",
    "<!-- n.tr <- nrow(dat.tr) -->\n",
    "<!-- set.seed(123456) -->\n",
    "<!-- for(j in 1:N) { -->\n",
    "<!--   ii <- sample(n.tr, replace=TRUE) -->\n",
    "<!--   tmp <- rpart(medv ~ ., data=dat.tr[ii, ], method='anova', control=con) -->\n",
    "<!--   myps[,j] <- predict(tmp, newdata=dat.te, type='vector') -->\n",
    "<!-- } -->\n",
    "<!-- pr.bagg <- rowMeans(myps) -->\n",
    "<!-- with(dat.te, mean( (medv - pr.bagg)^2 ) ) -->\n",
    "<!-- ``` -->\n",
    "<!-- Should we consider higher values of $N$? How about other -->\n",
    "<!-- training / test splits? Should we use CV instead?  -->\n",
    "<!-- Another split: -->\n",
    "<!-- ```{r anothersplit, fig.width=5, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE} -->\n",
    "<!-- set.seed(123) -->\n",
    "<!-- n <- nrow(Boston) -->\n",
    "<!-- ii <- sample(n, floor(n/4)) -->\n",
    "<!-- dat.te <- Boston[ ii, ] -->\n",
    "<!-- dat.tr <- Boston[ -ii, ] -->\n",
    "<!-- for(N in c(5, 10, 100, 1000)) { -->\n",
    "<!-- myps <- array(NA, dim=c(nrow(dat.te), N)) -->\n",
    "<!-- n.tr <- nrow(dat.tr) -->\n",
    "<!-- set.seed(123456) -->\n",
    "<!-- for(j in 1:N) { -->\n",
    "<!--   ii <- sample(n.tr, replace=TRUE) -->\n",
    "<!--   tmp <- rpart(medv ~ ., data=dat.tr[ii, ], method='anova', control=con) -->\n",
    "<!--   myps[,j] <- predict(tmp, newdata=dat.te, type='vector') -->\n",
    "<!-- } -->\n",
    "<!-- pr.bagg <- rowMeans(myps) -->\n",
    "<!-- print(c(N, with(dat.te, mean( (medv - pr.bagg)^2 ) ))) -->\n",
    "<!-- } -->\n",
    "<!-- ``` -->\n",
    "<!-- Similar conclusion: increasing $N$ helps, but the improvement  -->\n",
    "<!-- becomes smaller, while the computational cost keeps increasing.  -->\n",
    "<!-- ### Bagging a regression spline -->\n",
    "<!-- Bagging does not provide much of an advantage when applied to linear -->\n",
    "<!-- predictors (can you explain why?) Nevertheless, let us try it on the `lidar` data,  -->\n",
    "<!-- which, as we did before, we randomly split into a training and test set: -->\n",
    "<!-- ```{r bagsplines, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n",
    "<!-- data(lidar, package='SemiPar') -->\n",
    "<!-- set.seed(123456) -->\n",
    "<!-- n <- nrow(lidar) -->\n",
    "<!-- ii <- sample(n, floor(n/5)) -->\n",
    "<!-- lid.te <- lidar[ ii, ] -->\n",
    "<!-- lid.tr <- lidar[ -ii, ] -->\n",
    "<!-- ``` -->\n",
    "<!-- Now fit a cubic spline, and estimate the MSPE using the test set: -->\n",
    "<!-- ```{r bagsplines2, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n",
    "<!-- library(splines) -->\n",
    "<!-- a <- lm(logratio ~ bs(x=range, df=10, degree=3), data=lid.tr)  -->\n",
    "<!-- oo <- order(lid.tr$range) -->\n",
    "<!-- pr.of <- predict(a, newdata=lid.te) -->\n",
    "<!-- mean( (lid.te$logratio - pr.of)^2 ) -->\n",
    "<!-- ``` -->\n",
    "<!-- We build an ensemble of 10 fits and estimate the corresponding -->\n",
    "<!-- MSPE using the test set: -->\n",
    "<!-- ```{r bagsplines3, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n",
    "<!-- N <- 10 # 5 500 1500 -->\n",
    "<!-- myps <- matrix(NA, nrow(lid.te), N) -->\n",
    "<!-- set.seed(123456) -->\n",
    "<!-- n.tr <- nrow(lid.tr) -->\n",
    "<!-- for(i in 1:N) { -->\n",
    "<!--   ii <- sample(n.tr, replace=TRUE) -->\n",
    "<!--   a.b <- lm(logratio ~ bs(x=range, df=10, degree=3), data=lid.tr[ii,])  -->\n",
    "<!--   myps[,i] <- predict(a.b, newdata=lid.te) -->\n",
    "<!-- } -->\n",
    "<!-- pr.ba <- rowMeans(myps)# , na.rm=TRUE) -->\n",
    "<!-- mean( (lid.te$logratio - pr.ba)^2 ) -->\n",
    "<!-- ``` -->\n",
    "<!-- Using smoothing splines? -->\n",
    "<!-- ```{r bagsmooth, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n",
    "<!-- a <- smooth.spline(x = lid.tr$range, y = lid.tr$logratio, cv = TRUE, all.knots = TRUE) -->\n",
    "<!-- pr.of <- predict(a, x=lid.te$range)$y -->\n",
    "<!-- mean( (lid.te$logratio - pr.of)^2 ) -->\n",
    "<!-- ``` -->\n",
    "<!-- Using ensemble of 10: -->\n",
    "<!-- ```{r bagsmooth2, fig.width=5, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE} -->\n",
    "<!-- N <- 10 # 5 500 1500 -->\n",
    "<!-- myps <- matrix(NA, nrow(lid.te), N) -->\n",
    "<!-- set.seed(123456) -->\n",
    "<!-- n.tr <- nrow(lid.tr) -->\n",
    "<!-- for(i in 1:N) { -->\n",
    "<!--   ii <- sample(n.tr, replace=TRUE) -->\n",
    "<!--   a.b <- smooth.spline(x = lid.tr$range[ii], y = lid.tr$logratio[ii], cv = TRUE, all.knots = TRUE) -->\n",
    "<!--   myps[,i] <- predict(a.b, x=lid.te$range)$y -->\n",
    "<!-- } -->\n",
    "<!-- pr.ba <- rowMeans(myps)# , na.rm=TRUE) -->\n",
    "<!-- mean( (lid.te$logratio - pr.ba)^2 ) -->\n",
    "<!-- ``` -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
