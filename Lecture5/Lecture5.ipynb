{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LICENSE\n",
    "These notes are released under the \n",
    "\"Creative Commons Attribution-ShareAlike 4.0 International\" license. \n",
    "See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). \n",
    "\n",
    "## Lecture slides\n",
    "\n",
    "The lecture slides will be here.\n",
    "\n",
    "## Ridge regression \n",
    "\n",
    "Variable selection methods like stepwise can be highly variable. To illustrate this\n",
    "issue consider the following simple experiment. As in the previous lecture, \n",
    "we apply stepwise on 5 randomly selected folds of the data, and look at the\n",
    "models selected in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "airp <- read.table('../lecture1/rutgers-lib-30861_CSV-1.csv', header=TRUE, sep=',')\n",
    "library(MASS)\n",
    "k <- 5\n",
    "n <- nrow(airp)\n",
    "set.seed(123456)\n",
    "ii <- sample( (1:n) %% k + 1 )\n",
    "for(j in 1:k) {\n",
    "  x0 <- airp[ii != j, ]\n",
    "  null0 <- lm(MORT ~ 1, data=x0)\n",
    "  full0 <- lm(MORT ~ ., data=x0) # needed for stepwise\n",
    "  step.lm0 <- stepAIC(null0, scope=list(lower=null0, upper=full0), trace=FALSE)\n",
    "  print(formula(step.lm0)[[3]])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although many variables appear in more than one model, only\n",
    " `NONW` and `SO.` are in all of them, and \n",
    "`JANT` and `PREC` in 4 out of the 5. \n",
    "There are also several that appear in only one\n",
    "model (`HOUS`,\n",
    "`WWDRK` and \n",
    "`POPN`).\n",
    "<!-- `EDUC` 3 -->\n",
    "<!-- `JULT` in 3,  -->\n",
    "<!-- `DENS` in 2 -->\n",
    "<!-- and  -->\n",
    "This variability may in turn impact (negatively) the accuracy of the\n",
    "resulting predictions. \n",
    "\n",
    "A different approach to dealing with potentially correlated explanatory\n",
    "variables (with the goal of obtaining less variable / more accurate \n",
    "predictions) is to \"regularize\" the parameter estimates. In other words\n",
    "we modify the optimization problem that defines the parameter \n",
    "estimators (in the case of linear regression fits we tweak \n",
    "the least squares problem) to limit their size (in fact restricting \n",
    "them to be in a bounded and possibly small subset of the parameter\n",
    "space). \n",
    "\n",
    "The first proposal for a regularized / penalized estimator for \n",
    "linear regression models is Ridge Regression. \n",
    "We will use the function `glmnet` in package `glmnet` to\n",
    "compute the Ridge Regression estimator. Note that this \n",
    "function implements a larger family of regularized estimators,\n",
    "and in order to obtain a Ridge Regression estimator\n",
    "we need to set the argument `alpha = 0` of `glmnet()`. \n",
    "<!-- We use Ridge Regression with the air pollution data to obtain a -->\n",
    "<!-- more stable predictor. -->\n",
    "We also specify a range of possible values of the penalty \n",
    "coefficient (below we use a grid of 50 values between \n",
    "exp(-3) and exp(10))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "fig.height": "5",
     "fig.width": "5,",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "airp <- read.table('../lecture1/rutgers-lib-30861_CSV-1.csv', header=TRUE, sep=',')\n",
    "library(glmnet)\n",
    "# alpha = 0 - Ridge\n",
    "# alpha = 1 - LASSO\n",
    "y <- as.vector(airp$MORT)\n",
    "xm <- as.matrix(airp[, -16])\n",
    "lambdas <- exp( seq(-3, 10, length=50))\n",
    "a <- glmnet(x=xm, y=y, lambda=rev(lambdas),\n",
    "            family='gaussian', alpha=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned object contains the estimated regression coefficients for\n",
    "each possible value of the regularization parameter. We can look at\n",
    "them using the `plot` method for objects of class `glmnet` as \n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "fig.height": "5",
     "fig.width": "5,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "plot(a, xvar='lambda', label=TRUE, lwd=6, cex.axis=1.5, cex.lab=1.2, ylim=c(-20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the level of regularization\n",
    "\n",
    "Different values of the penalization parameter will typically yield estimators with\n",
    "varying predictive accuracies. To select a good level of regularization we estimate\n",
    "the MSPE of the estimator resulting from each value of the penalization parameter.\n",
    "One way to do this is to run K-fold cross validation for each value of\n",
    "the penalty. The `glmnet` package provides a built-in function to do this, \n",
    "and a `plot` method to display the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "fig.height": "5",
     "fig.width": "5,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# run 5-fold CV\n",
    "set.seed(123)\n",
    "tmp <- cv.glmnet(x=xm, y=y, lambda=lambdas, nfolds=5, alpha=0, family='gaussian')\n",
    "plot(tmp, lwd=6, cex.axis=1.5, cex.lab=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot the red dots are the estimated MSPE's for each value of the\n",
    "penalty, and the vertical lines mark plus/minus one (estimated) standard deviations (for each\n",
    "of those estimated MSPE's). The `plot` method will also mark the optimal value of\n",
    "the regularization parameter, and also the largest one for which the estimated MSPE\n",
    "is within 1-SD of the optimal. The latter is meant to provide a more regularized\n",
    "estimator with estimated MSPE within the error-margin of our estimated minimum.\n",
    "\n",
    "Note, however, that the above \"analysis\" is random (because of the intrinsic randomness of\n",
    "K-fold CV). If we run it again, we will most likely get different results. In many cases, \n",
    "however, the results will be qualitatively similar. If we run 5-fold CV again for this\n",
    "data get the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "echo": "FALSE",
     "fig.height": "5,",
     "fig.width": "5,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "set.seed(23)\n",
    "tmp <- cv.glmnet(x=xm, y=y, lambda=lambdas, nfolds=5, alpha=0, family='gaussian')\n",
    "plot(tmp, lwd=6, cex.axis=1.5, cex.lab=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both plots are similar, but not equal. It would be a good idea to repeat this\n",
    "a few times and explore how much variability is involved. If one were interested\n",
    "in selecting one value of the penalization parameter that was more stable than \n",
    "that obtained from a single 5-fold CV run, one could run it several times and\n",
    "take the average of the estimated optimal values. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "op.la <- 0\n",
    "for(j in 1:20) {\n",
    "  tmp <- cv.glmnet(x=xm, y=y, lambda=lambdas, nfolds=5, alpha=0, family='gaussian')\n",
    "  op.la <- op.la + tmp$lambda.min # tmp$lambda.1se\n",
    "}\n",
    "(op.la <- op.la / 20)\n",
    "log(op.la)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is reasonably close to the ones we saw in the plots above. \n",
    "\n",
    "## Comparing predictions \n",
    "\n",
    "We now run a cross-validation experiment to compare the \n",
    "MSPE of 3 models: the **full** model, the one\n",
    "selected by **stepwise** and the **ridge regression**\n",
    "one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "fig.height": "5",
     "fig.width": "5,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "n <- nrow(xm)\n",
    "k <- 5\n",
    "ii <- (1:n) %% k + 1\n",
    "set.seed(123)\n",
    "N <- 100\n",
    "mspe.st <- mspe.ri <- mspe.f <- rep(0, N)\n",
    "for(i in 1:N) {\n",
    "  ii <- sample(ii)\n",
    "  pr.f <- pr.ri <- pr.st <- rep(0, n)\n",
    "  for(j in 1:k) {\n",
    "    tmp.ri <- cv.glmnet(x=xm[ii != j, ], y=y[ii != j], lambda=lambdas,\n",
    "                        nfolds=5, alpha=0, family='gaussian')\n",
    "    null <- lm(MORT ~ 1, data=airp[ii != j, ])\n",
    "    full <- lm(MORT ~ ., data=airp[ii != j, ])\n",
    "    tmp.st <- stepAIC(null, scope=list(lower=null, upper=full), trace=0)\n",
    "    pr.ri[ ii == j ] <- predict(tmp.ri, s='lambda.min', newx=xm[ii==j,])\n",
    "    pr.st[ ii == j ] <- predict(tmp.st, newdata=airp[ii==j,])\n",
    "    pr.f[ ii == j ] <- predict(full, newdata=airp[ii==j,])\n",
    "  }\n",
    "  mspe.ri[i] <- mean( (airp$MORT - pr.ri)^2 )\n",
    "  mspe.st[i] <- mean( (airp$MORT - pr.st)^2 )\n",
    "  mspe.f[i] <- mean( (airp$MORT - pr.f)^2 )\n",
    "}\n",
    "boxplot(mspe.ri, mspe.st, mspe.f, names=c('Ridge', 'Stepwise', 'Full'), \n",
    "        col=c('gray80', 'tomato', 'springgreen'), cex.axis=1.5, cex.lab=1.5, \n",
    "        cex.main=2, ylim=c(1300, 3000))\n",
    "mtext(expression(hat(MSPE)), side=2, line=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more stable Ridge Regression?\n",
    "\n",
    "Here we try to obtain a ridge regression estimator\n",
    "with more stable predictions by using the \n",
    "average optimal penalty value using 20 runs. \n",
    "The improvement does not appear to be \n",
    "substantial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "echo": "FALSE,",
     "fig.height": "5",
     "fig.width": "5,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "n <- nrow(xm)\n",
    "k <- 5\n",
    "ii <- (1:n) %% k + 1\n",
    "set.seed(123)\n",
    "N <- 100\n",
    "mspe.ri2 <- rep(0, N)\n",
    "for(i in 1:N) {\n",
    "  ii <- sample(ii)\n",
    "  pr.ri2  <- rep(0, n)\n",
    "  for(j in 1:k) {\n",
    "    op.la <- 0\n",
    "    for(h in 1:20) {\n",
    "      tmp <- cv.glmnet(x=xm, y=y, lambda=lambdas, nfolds=5, alpha=0, family='gaussian')\n",
    "      op.la <- op.la + tmp$lambda.min # tmp$lambda.1se\n",
    "    }\n",
    "    op.la <- op.la / 20\n",
    "    tmp.ri <- cv.glmnet(x=xm[ii != j, ], y=y[ii != j], lambda=lambdas, nfolds=5, \n",
    "                        alpha=0, family='gaussian')\n",
    "    pr.ri2[ ii == j ] <- predict(tmp.ri, s=op.la, newx=xm[ii==j,])\n",
    "  }\n",
    "  mspe.ri2[i] <- mean( (airp$MORT - pr.ri2)^2 )\n",
    "}\n",
    "boxplot(mspe.ri2, mspe.ri, mspe.st, mspe.f, names=c('Stable R', 'Ridge', 'Stepwise', 'Full'),\n",
    "        col=c('steelblue', 'gray80', 'tomato', 'springgreen'), cex.axis=1.5, cex.lab=1.5, \n",
    "        cex.main=2, ylim=c(1300, 3000))\n",
    "mtext(expression(hat(MSPE)), side=2, line=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example where one may not need to select variables\n",
    "\n",
    "In some cases one may not need to select a subset of explanatory\n",
    "variables, and in fact, doing so may affect negatively the accuracy of\n",
    "the resulting predictions. In what follows we discuss such an example. \n",
    "Consider the credit card data set that contains information\n",
    "on credit card users. The interest is in predicting the \n",
    "balance carried by a client. We first load the data, and to\n",
    "simplify the presentation here we consider only the numerical\n",
    "explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- read.table('Credit.csv', sep=',', header=TRUE, row.names=1)\n",
    "x <- x[, c(1:6, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 available covariates, and a stepwise search selects \n",
    "a model with 5 of them (discarding `Education`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE",
     "warning": "FALSE,"
    }
   },
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "null <- lm(Balance ~ 1, data=x)\n",
    "full <- lm(Balance ~ ., data=x)\n",
    "(tmp.st <- stepAIC(null, scope=list(lower=null, upper=full), trace=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an easy exercise to check that the MSPE of this\n",
    "smaller model is in fact worse than the one for the **full** one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "echo": "FALSE",
     "fig.height": "5,",
     "fig.width": "5,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "n <- nrow(x)\n",
    "k <- 5\n",
    "ii <- (1:n) %% k + 1\n",
    "set.seed(123)\n",
    "N <- 100\n",
    "mspe.st <- mspe.f <- rep(0, N)\n",
    "for(i in 1:N) {\n",
    "  ii <- sample(ii)\n",
    "  pr.f <- pr.st <- rep(0, n)\n",
    "  for(j in 1:k) {\n",
    "    null <- lm(Balance ~ 1, data=x[ii != j, ])\n",
    "    full <- lm(Balance ~ ., data=x[ii != j, ])\n",
    "    tmp.st <- stepAIC(null, scope=list(lower=null, upper=full), trace=0)\n",
    "    pr.st[ ii == j ] <- predict(tmp.st, newdata=x[ii==j,])\n",
    "    pr.f[ ii == j ] <- predict(full, newdata=x[ii==j,])\n",
    "  }\n",
    "  mspe.st[i] <- mean( (x$Balance - pr.st)^2 )\n",
    "  mspe.f[i] <- mean( (x$Balance - pr.f)^2 )\n",
    "}\n",
    "boxplot(mspe.st, mspe.f, names=c('Stepwise', 'Full'),\n",
    "        col=c('tomato', 'springgreen'), cex.axis=1.5,\n",
    "        cex.lab=1.5, cex.main=2)\n",
    "mtext(expression(hat(MSPE)), side=2, line=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ridge regression instead of stepwise to prevent \n",
    "the negative effect of possible correlations among the\n",
    "covariates yields a slight improvement (over the **full** model), \n",
    "but it is not clear the gain is worth the effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "echo": "FALSE",
     "fig.height": "5,",
     "fig.width": "5,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "library(glmnet)\n",
    "y <- as.vector(x$Balance)\n",
    "xm <- as.matrix(x[, -7])\n",
    "lambdas <- exp( seq(-3, 10, length=50))\n",
    "n <- nrow(xm)\n",
    "k <- 5\n",
    "ii <- (1:n) %% k + 1\n",
    "set.seed(123)\n",
    "N <- 100\n",
    "mspe.st <- mspe.ri <- mspe.f <- rep(0, N)\n",
    "for(i in 1:N) {\n",
    "  ii <- sample(ii)\n",
    "  pr.f <- pr.ri <- pr.st <- rep(0, n)\n",
    "  for(j in 1:k) {\n",
    "    tmp.ri <- cv.glmnet(x=xm[ii != j, ], y=y[ii != j], lambda=lambdas,\n",
    "                     nfolds=5, alpha=0, family='gaussian')\n",
    "    null <- lm(Balance ~ 1, data=x[ii != j, ])\n",
    "    full <- lm(Balance ~ ., data=x[ii != j, ])\n",
    "    tmp.st <- stepAIC(null, scope=list(lower=null, upper=full), trace=0)\n",
    "    pr.ri[ ii == j ] <- predict(tmp.ri, s='lambda.min', newx=xm[ii==j,])\n",
    "    pr.st[ ii == j ] <- predict(tmp.st, newdata=x[ii==j,])\n",
    "    pr.f[ ii == j ] <- predict(full, newdata=x[ii==j,])\n",
    "  }\n",
    "  mspe.ri[i] <- mean( (x$Balance - pr.ri)^2 )\n",
    "  mspe.st[i] <- mean( (x$Balance - pr.st)^2 )\n",
    "  mspe.f[i] <- mean( (x$Balance - pr.f)^2 )\n",
    "}\n",
    "boxplot(mspe.ri, mspe.st, mspe.f, names=c('Ridge', 'Stepwise',\n",
    "'Full'), col=c('gray80', 'tomato', 'springgreen'), cex.axis=1.5,\n",
    "cex.lab=1.5, cex.main=2)\n",
    "mtext(expression(hat(MSPE)), side=2, line=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An important limitation of Ridge Regression\n",
    "\n",
    "Ridge Regression typically yields estimators with more accurate (less variable) \n",
    "predictions, specially when there is noticeable correlation among covariates. \n",
    "However, it is important to note that Ridge Regression does not select \n",
    "variables, and in that sense it does not \"replace\" methods like stepwise when\n",
    "the interest is in using a smaller number of explanatory variables. Furthermore,\n",
    "the interpretation of the Ridge Regression coefficient estimates is \n",
    "generally difficult. LASSO regression estimates were proposed to \n",
    "address these two issues (more stable predictions when correlated \n",
    "covariates are present **and** variable selection) simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
