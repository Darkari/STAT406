{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT406 - Lecture 16 notes\n",
    "================\n",
    "Matias Salibian-Barrera\n",
    "2018-11-06\n",
    "\n",
    "LICENSE\n",
    "-------\n",
    "\n",
    "These notes are released under the \"Creative Commons Attribution-ShareAlike 4.0 International\" license. See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/) and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Lecture slides\n",
    "--------------\n",
    "\n",
    "The lecture slides are [here](STAT406-18-lecture-16.pdf).\n",
    "\n",
    "Random Forests\n",
    "==============\n",
    "\n",
    "Even though using a *bagged* ensemble of trees usually results in a more stable predictor / classifier, a better ensemble can be improved by training each of its members in a careful way. The main idea is to try to reduce the (conditional) potential correlation among the predictions of the bagged trees, as discussed in class. Each of the bootstrap trees in the ensemble is grown using only a randomly selected set of features when partitioning each node. More specifically, at each node only a random subset of explanatory variables is considered to determine the optimal split. These randomly chosen features are selected independently at each node as the tree is being constructed.\n",
    "\n",
    "To train a Random Forest in `R` we use the funtion `randomForest` from the package with the same name. The syntax is the same as that of `rpart`, but the tuning parameters for each of the *trees* in the *forest* are different from `rpart`. Refer to the help page if you need to modify them.\n",
    "\n",
    "We load and prepare the admissions data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm <- read.table('../Lecture14/T11-6.DAT', header=FALSE)\n",
    "mm$V3 <- as.factor(mm$V3)\n",
    "mm[,2] <- mm[,2] / 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and train a Random Forest with 500 trees and using all the default tuning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(randomForest)\n",
    "a.rf <- randomForest(V3~V1+V2, data=mm, ntree=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions can be obtained using the `predict` method, as usual, when you specify the `newdata` argument. Refer to the help page of `predict.randomForest` for details on the different behaviour of `predict` for Random Forest objects when the argument `newdata` is either present or missing.\n",
    "\n",
    "To visualize the predicted classes obtained with a Random Forest on our example data, we compute the corresponding predicted conditional class probabilities on the same grid used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa <- seq(2, 5, length=200)\n",
    "bb <- seq(2, 5, length=200)\n",
    "dd <- expand.grid(aa, bb)\n",
    "names(dd) <- names(mm)[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated conditional probabilities for class *red* are shown in the plot below (how are these estimated conditional probabilities computed exactly?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.rf <- predict(a.rf, newdata=dd, type='prob')\n",
    "filled.contour(aa, bb, matrix(pp.rf[,1], 200, 200), col=terrain.colors(20), xlab='GPA', ylab='GMAT', \n",
    "               plot.axes={axis(1); axis(2)},\n",
    "               panel.last={points(mm[,-3], pch=19, cex=1.5, col=c(\"red\", \"blue\", \"green\")[mm[,3]])}\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very interesting exercise would be to train a Random Forest on the perturbed data (in `mm2`) and verify that the predicted conditional probabilities do not change much, as was the case for the bagged classifier.\n",
    "\n",
    "### Another example\n",
    "\n",
    "We will now use a more interesting example. The ISOLET data, available here: <http://archive.ics.uci.edu/ml/datasets/ISOLET>, contains data on sound recordings of 150 speakers saying each letter of the alphabet (twice). See the original source for more details. Since the full data set is rather large, here we only use the subset corresponding to the observations for the letters **C** and **Z**.\n",
    "\n",
    "We first load the training and test data sets, and force the response variable to be categorical, so that the `R` implementations of the different predictors we will use below will build classifiers and not their regression counterparts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr <- read.table('../Lecture15/isolet-train-c-z.data', sep=',')\n",
    "xte <- read.table('../Lecture15/isolet-test-c-z.data', sep=',') \n",
    "xtr$V618 <- as.factor(xtr$V618)\n",
    "xte$V618 <- as.factor(xte$V618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a Random Forest we use the function `randomForest` in the package of the same name. The code underlying this package was originally written by Leo Breiman. We first train a Random Forest, using all the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(randomForest)\n",
    "set.seed(123)\n",
    "( a.rf <- randomForest(V618 ~ ., data=xtr, ntree=500) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check its performance on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.rf <- predict(a.rf, newdata=xte, type='response')\n",
    "table(p.rf, xte$V618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Random Forest only makes one mistake out of 120 (approx 0.8%) observations in the test set. However, the OOB error rate estimate is slightly over 2%. The next plot shows the evolution of the OOB error rate estimate as a function of the number of classifiers in the ensemble (trees in the forest). Note that 500 trees appears to be a reasonable forest size, in the sense thate the OOB error rate estimate is stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(a.rf, lwd=3, lty=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a test set instead of OBB\n",
    "\n",
    "Given that in this case we do have a test set, we can use it to monitor the error rate (instead of using the OOB error estimates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.train <- model.matrix(V618 ~ ., data=xtr)\n",
    "y.train <- xtr$V618\n",
    "x.test <- model.matrix(V618 ~ ., data=xte)\n",
    "y.test <- xte$V618\n",
    "set.seed(123)\n",
    "a.rf <- randomForest(x=x.train, y=y.train, xtest=x.test, ytest=y.test, ntree=500) \n",
    "test.err <- a.rf$test$err.rate\n",
    "ma <- max(c(test.err))\n",
    "plot(test.err[, 2], lwd=2, lty=1, col='red', type='l', ylim=c(0, max(c(0, ma))))\n",
    "lines(test.err[, 3], lwd=2, lty=1, col='green')\n",
    "lines(test.err[, 1], lwd=2, lty=1, col='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the help page for the `plot` method for objects of class `randomForest`, the following plot should show both error rates (OOB plus those on the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(a.rf, lwd=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature sequencing / Variable ranking\n",
    "\n",
    "To explore which variables were used in the forest, and also, their importance rank as discussed in class, we can use the function `varImpPlot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varImpPlot(a.rf, n.var=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing RF with other classifiers\n",
    "\n",
    "We now compare the Random Forest with some of the other classifiers we saw in class, using their classification error rate on the test set as our comparison measure. We first start with K-NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(class)\n",
    "u1 <- knn(train=xtr[, -618], test=xte[, -618], cl=xtr[, 618], k = 1)\n",
    "table(u1, xte$V618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u5 <- knn(train=xtr[, -618], test=xte[, -618], cl=xtr[, 618], k = 5)\n",
    "table(u5, xte$V618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u10 <- knn(train=xtr[, -618], test=xte[, -618], cl=xtr[, 618], k = 10)\n",
    "table(u10, xte$V618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u20 <- knn(train=xtr[, -618], test=xte[, -618], cl=xtr[, 618], k = 20)\n",
    "table(u20, xte$V618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u50 <- knn(train=xtr[, -618], test=xte[, -618], cl=xtr[, 618], k = 50)\n",
    "table(u50, xte$V618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use logistic regression we first create a new variable that is 1 for the letter **C** and 0 for the letter **Z**, and use it as our response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr$V619 <- as.numeric(xtr$V618==3)\n",
    "d.glm <- glm(V619 ~ . - V618, data=xtr, family=binomial)\n",
    "pr.glm <- as.numeric( predict(d.glm, newdata=xte, type='response') >  0.5 )\n",
    "table(pr.glm, xte$V618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question for the reader: why do you think this classifier's performance is so disappointing?\n",
    "\n",
    "It is interesting to see how a simple LDA classifier does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "xtr$V619 <- NULL\n",
    "d.lda <- lda(V618 ~ . , data=xtr)\n",
    "pr.lda <- predict(d.lda, newdata=xte)$class\n",
    "table(pr.lda, xte$V618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that a carefully built classification tree performs remarkably well, only using 3 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "my.c <- rpart.control(minsplit=5, cp=1e-8, xval=10)\n",
    "set.seed(987)\n",
    "a.tree <- rpart(V618 ~ ., data=xtr, method='class', parms=list(split='information'), control=my.c)\n",
    "cp <- a.tree$cptable[which.min(a.tree$cptable[,\"xerror\"]),\"CP\"]\n",
    "a.tp <- prune(a.tree, cp=cp)\n",
    "p.t <- predict(a.tp, newdata=xte, type='vector')\n",
    "table(p.t, xte$V618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that if you train a single classification tree with the default values for the stopping criterion tuning parameters, the tree also uses only 3 features, but its classification error rate on the test set is larger than that of the pruned one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(987)\n",
    "a2.tree <- rpart(V618 ~ ., data=xtr, method='class', parms=list(split='information'))\n",
    "p2.t <- predict(a2.tree, newdata=xte, type='vector')\n",
    "table(p2.t, xte$V618)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
