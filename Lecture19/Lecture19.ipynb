{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT406 - Lecture 19 notes\n",
    "================\n",
    "Matias Salibian-Barrera\n",
    "2018-11-20\n",
    "\n",
    "LICENSE\n",
    "-------\n",
    "\n",
    "These notes are released under the \"Creative Commons Attribution-ShareAlike 4.0 International\" license. See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/) and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Lecture slides\n",
    "--------------\n",
    "\n",
    "Lecture slides are [here](STAT406-18-lecture-19.pdf).\n",
    "\n",
    "Unsupervised learning\n",
    "=====================\n",
    "\n",
    "Unsupervised learning methods differ from the supervised ones we have studied so far in that there is no response variable. The objective is not related to prediction but rather to the identification of different possible structures that may be present in the data. For example, one may be interested in determining whether the observations are \"grouped\" in some way (clustering), or if the data can be efficiently represented using fewer variables or features (dimension reduction).\n",
    "\n",
    "Many of these methods do not rely on any probabilistic model, and thus there may not be a clear *target* to be estimated or approximated. As a consequence, the conclusions that can be reached from this type of analyses are often of an exploratory nature.\n",
    "\n",
    "Clustering\n",
    "----------\n",
    "\n",
    "A large class of unsupervised learning methods is collectively called *clustering*. The objective can be described as identifying different groups of observations that are closer to each other (\"clustered\") than to those of the other groups. The data consist of *n* observations *X*<sub>1</sub>, *X*<sub>2</sub>, ..., *X*<sub>*n*</sub>, each with *p* features. In general, the number of groups is unknown and needs to be determined from the data.\n",
    "\n",
    "In this course we will discuss both model-free and model-based clustering methods. In the first group we will present K-means (and other related methods), and hierarchical (agglomerative) clustering methods. Model-based clustering is based on the assumption that the data is a random sample, and that the distribution of the vector of features **X** is a combination of different distributions (technically: a *mixture model*). In the latter case, for observations belonging to a group, the vector **X** is assumed to have a distribution in a specific (generally parametric) family. Some of these model-based methods treat the group labels as missing (unobserved) responses, and rely on the assumed model to infer those missing labels.\n",
    "\n",
    "### K-means, K-means++, K-medoids\n",
    "\n",
    "Probably the most intuitive and easier to explain unsupervised clustering algorithm is K-means (and its variants *K-means++* and K-medoids, a.k.a. *pam*, partition around medoids). The specifics of the K-means algorithm were discussed in class. Here we will illustrate its use on a few examples.\n",
    "\n",
    "#### UN votes example.\n",
    "\n",
    "These data contain the historical voting patterns of United Nations members. More details can be found here:\n",
    "\n",
    "> Voeten, Erik; Strezhnev, Anton; Bailey, Michael, 2009, \"United Nations General Assembly Voting Data\", <http://hdl.handle.net/1902.1/12379>, Harvard Dataverse, V11\n",
    "\n",
    "The UN was founded in 1946 and it contains 193 member states. The data include only \"important\" votes, as classified by the U.S. State Department. The votes for each country were coded as follows: Yes (1), Abstain (2), No (3), Absent (8), Not a Member (9). There were 368 important votes, and 77 countries voted in at least 95% of these. We focus on these UN members. Our goal is to explore whether voting patterns reflect political alignments, and also whether countries vote along known political blocks. Our data consists of 77 observations with 368 variables each. More information on these data can be found [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/12379).\n",
    "\n",
    "The dataset is organized by vote (resolution), one per row, and its columns contain the corresponding vote of each country (one country per column). We first read the data, and limit ourselves to resolutions where every country voted without missing votes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X <- read.table(file='unvotes.csv', sep=',', row.names=1, header=TRUE)\n",
    "X2 <- X[complete.cases(X),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute a K-means partition using the function `kmeans` with *K = 5*, and look at the resulting groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "b <- kmeans(t(X2), centers=5, iter.max=20, nstart=1)\n",
    "table(b$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run `kmeans` again, we might get a different partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b <- kmeans(t(X2), centers=5, iter.max=20, nstart=1)\n",
    "table(b$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to consider a large number of random starts and take the **best** found solution (what does *best* mean in this context? in other words, how does the algorithm decide which one is the solution it should return?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the best solution out of 1000 random starts\n",
    "b <- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1000)\n",
    "split(colnames(X2), b$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be better to look at the groups on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rworldmap)\n",
    "library(countrycode)\n",
    "these <- countrycode(colnames(X2), \"country.name\", \"iso3c\")\n",
    "malDF <- data.frame(country = these, cluster = b$cluster)\n",
    "# malDF is a data.frame with the ISO3 country names plus a variable to merge\n",
    "# to the map data\n",
    "\n",
    "# This line will join your malDF data.frame to the country map data\n",
    "malMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors()[grep('blue', colors())] fill the space on the graphical device\n",
    "par(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\n",
    "mapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", \n",
    "    missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \n",
    "        \"hotpink\", \"tomato\", \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this partition with the one we obtain using PAM (K-medoids), which is implemented in the function `pam` of the package `cluster`. Recall from the discussion in class that `pam` does not need to manipulate the actual observations, only its pairwise distances (or dissimilarities). In this case we use Euclidean distances, but it may be interesting to explore other distances, particulary in light of the categorical nature of the data. Furthermore, to obtain clusters that may be easier to interpret we use `K = 3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(cluster)\n",
    "# Use Euclidean distances\n",
    "d <- dist(t(X))\n",
    "# what happens with missing values?\n",
    "set.seed(123)\n",
    "a <- pam(d, k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the resulting groups with those of *K-means*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b <- kmeans(t(X2), centers = 3, iter.max = 20, nstart = 1000)\n",
    "table(a$clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(b$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An better visualization is done using the map. interesting We plot the 3 groups found by `pam` on the map, followed by those found by K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "these <- countrycode(colnames(X), \"country.name\", \"iso3c\")\n",
    "malDF <- data.frame(country = these, cluster = a$clustering)\n",
    "malMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\n",
    "mapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", \n",
    "    missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \n",
    "        \"hotpink\", \"tomato\", \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "these <- countrycode(colnames(X2), \"country.name\", \"iso3c\")\n",
    "malDF <- data.frame(country = these, cluster = b$cluster)\n",
    "malMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\n",
    "mapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", \n",
    "    missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"yellow\", \n",
    "        \"tomato\", \"blueviolet\"), oceanCol = \"dodgerblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we use the L\\_1 norm instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d <- dist(t(X), method = \"manhattan\")\n",
    "set.seed(123)\n",
    "a <- pam(d, k = 3)\n",
    "these <- countrycode(colnames(X), \"country.name\", \"iso3c\")\n",
    "malDF <- data.frame(country = these, cluster = a$clustering)\n",
    "malMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\n",
    "mapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", \n",
    "    missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \n",
    "        \"hotpink\", \"tomato\", \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, since the data set does not include a *true label*, the comparison between the different results is somewhat subjective, and it often relies on the knowledge of the subject matter experts. In our example above, this would mean asking the opinion of a political scientist as to whether these groupings correspond to known international political blocks or alignments.\n",
    "\n",
    "#### Breweries\n",
    "\n",
    "In this example beer drinkers were asked to rate 9 breweries one 26 attributes, e.g. whether this brewery has a rich tradition; or whether it makes very good pilsner beer, etc. For each of these questions, the *judges* reported a score on a 6-point scale ranging from 1: \"not true at all\" to 6: \"very true\". The data are in the file `breweries.dat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- read.table('breweries.dat', header=FALSE)\n",
    "x <- t(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purposes we use the *L*<sub>1</sub> distance and the PAM clustering method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d <- dist(x, method='manhattan')\n",
    "set.seed(123)\n",
    "a <- pam(d, k=3)\n",
    "table(a$clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the strength of these cluster partition we use the silhouette plot discussed in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since other distances may produce different partitions, an interesting exercise would be to compare the above clusters with those found using the Euclidean or *L*<sub>∞</sub> norms, for example.\n",
    "\n",
    "#### Cancer example\n",
    "\n",
    "This data contains gene expression levels for 6830 genes (rows) for 64 cell samples (columns). More information can be found here: <http://genome-www.stanford.edu/nci60/>. The data are included in the `ElemStatLearn` package, and also available on-line: <https://web.stanford.edu/~hastie/ElemStatLearn/>.\n",
    "\n",
    "We will use K-means to identify 8 possible clusters among the 64 cell samples. As discussed in class this exercise can (perhaps more interestingly) be formulated in terms of *feature selection*. We load the data and use K-means to find 8 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(nci, package = \"ElemStatLearn\")\n",
    "ncit <- t(nci)\n",
    "set.seed(31)\n",
    "a <- kmeans(ncit, centers = 8, iter.max = 5000, nstart = 100)\n",
    "table(a$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this application we do know the group to which each observation belongs (its cancer type). We can look at the cancer types that have been grouped together in each of the 8 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(split(colnames(nci), a$cluster), table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that clusters 2, 3, 5, 6 and 7 are dominated by one type of cancer. Similarly, almost all melanoma and renal samples are in clusters 7 and 6, respectively, while all CNS samples are in cluster 2. Cluster 4 is harder to interpret. Although all ovarian cancer samples are in this cluster, it also contains 2/3 of the NSCLC samples. It may be of interest to compare these results with those using different numbers of clusters.\n",
    "\n",
    "Finally, we will compare this partition (obtained with the usual K-means algorithm started from `100` random initial configurations) with the one found by K++ as implemented in `flexclust::kcca`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(flexclust)\n",
    "set.seed(123)\n",
    "kpp <- kcca(ncit, k = 8, family = kccaFamily(\"kmeans\"), control = list(initcent = \"kmeanspp\"))\n",
    "table(kpp@cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the one obtained with `kmeans` was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(a$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the two partitions are quite different. Furthermore, note that there is still a lot of randomness in the solution returned by K++. If we run it again with a different pseudo-random number generating seed we obtain a very different partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(987)\n",
    "table(kcca(ncit, k = 8, family = kccaFamily(\"kmeans\"), control = list(initcent = \"kmeanspp\"))@cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very good exercise would be to determine which of these different partitions (the ones found with K++ and the one returned by 100 random starts of K-means) is better in terms of the objective function being optimized to find the clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
