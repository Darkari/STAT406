---
title: "Model-based classification (LDA, QDA, etc.)"
author: "Matias Salibian"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: 
    latex_engine: xelatex
mainfont: Arial
fontsize: 12pt 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=6, message=FALSE, warning=FALSE)
```

## Model-based classification

Consider the following setting for a classification problem: 
there are $p$ explanatory variables (features), collected in 
a vector $\mathbf{X} \in \mathbb{R}^p$, along with a 
categorical variable $G$ that takes one of $K$ possible 
values in the set ${\cal C} = \left\{ c_1, c_2, \ldots, c_K \right\}$. Given a training
set $(g_1, \mathbf{x}_1)$, 
$(g_2, \mathbf{x}_2)$, $\ldots$, 
$(g_n, \mathbf{x}_n)$ we are interested in constructing
a classifier (i.e. a function $\hat{g} : \mathbf{R}^p \to {\cal C}$)
with good prediction properties. 

As we discussed in class, given a point $\mathbf{x}_0$,
the optimal classification (with respect to the 0-1 loss)
picks the class $c_j$ with the highest conditional probability 
of occurring given $\mathbf{x}_0$. In symbols:
$$
\hat{g}(\mathbf{x}_0) \, = \, \arg \max_{c_\ell \in {\cal C}} \ P \left( \left. G = c_\ell \right| \, \mathbf{X} = \mathbf{x}_0 \right) \, .
$$
If we know (can model reasonably) and can estimate all the 
conditional probabilities  
$P( G = c_\ell | \mathbf{X} = \mathbf{x})$ 
for all possible values of the features $\mathbf{x} \in \mathbb{R}^p$
and all classes $c_\ell \in {\cal C}$, 
then we can estimate the optimal (with respect to the 0-1 loss) classifier.
One way to model (and estimate) the above conditional probabilities is
to do it indirectly by modeling the distribution of the vector of
features $\mathbf{X}$ for each class, in symbols, the (conditional)
distribution $\mathbf{X} | G = c_\ell$. If $f(\mathbf{x} | G = c_j)$
is the conditional density (or pmf) of the vector of explanatory 
variables given that the response is $c_j$, then we always have:
$$
P \left( \left. G = c_j \right| \, \mathbf{X} = \mathbf{x}_0 \right) \, = \, \frac{ f( \, \mathbf{x}_0 \, | \, G = c_j) P( G = c_j )}{ \sum_{\ell = 1}^K \, f( \, \mathbf{x}_0 | G = c_\ell) P( G = c_\ell )} \, . 
$$
Hence, to find the value $c_i$ with the largest 
$P( G = c_i | \mathbf{X} = \mathbf{x}_0)$ 
we only need to compute the numerators in the right hand side above
for each $c_j \in {\cal C}$ and select the class with the largest value. 
We now look at a simple example of the above general approach.

### An example: LDA

One of the simplest models for 
$\mathbf{X} | G = c_\ell$ is a (multivariate) normal (Gaussian)
distribution with mean $\mathbf{\mu}_\ell \in \mathbb{R}^p$
and covariance matrix $\mathbf{\Sigma} \in \mathbb{R}^{p \times p}$
(the same for all $c_\ell \in {\cal C}$). In symbols:
$$
\mathbf{X} | G = c_\ell \ \sim \ {\cal N}\left( 
\mathbf{\mu}_\ell, 
\mathbf{\Sigma} \right) \, .
$$
The density function for a ${\cal N} \left( \mathbf{\mu}, \mathbf{\Sigma} \right)$ distribution is given by
$$
f( \mathbf{x} ) \, = \, \frac{1}{(2 \pi)^p} \, \frac{1}{| \mathbf{\Sigma} |^{p/2}} \, \exp \left( (\mathbf{x} - \mathbf{\mu})^\top \, \mathbf{\Sigma}^{-1} \, 
(\mathbf{x} - \mathbf{\mu}) \right) \, .
$$
In order to use all this machinery we need to estimate the density functions 
for all the ${\cal N} \left( \mathbf{\mu}_j, \mathbf{\Sigma} \right)$ 
distributions, and then, given a point $\mathbf{x}_0$, evaluate them and
choose the class with the largest value of 
$f( \, \mathbf{x}_0 \, | \, G = c_j) P( G = c_j )$. 

To estimate the above densities we need to estimate the following
parameters:

- $\mathbf{\mu}_i$, $i = 1, \ldots, K$;
- $\mathbf{\Sigma}$;

and also the following probabilities:

- $P( G = c_j )$, $j = 1, \ldots, K$. 

The usual estimators for the mean vectors are the sample means of
the features in each class in the training set, and the common
covariance matrix (across classes) can be estimated by taking
a weighted average of the sample covariance matrix in each class
using the data in the training set. Finally, the probabilities of 
each class can be estimated with the observed frequency of each
class in the training set. 

In symbols, we have:

- For class $c_i \in {\cal C}$ the estimated vector of means:
$$
\hat{\mathbf{\mu}}_i \, = \, \frac{1}{n_i} \sum_{j \in {\cal N}_i} \mathbf{x}_j \, ,
$$
where ${\cal N}_i$ is the set of indices of 
observations in the training set with $g = c_i$, and $n_i$ is the cardinal
of that set. 
- The common matrix $\mathbf{\Sigma}$ can be estimated with
$$
\widehat{\mathbf{\Sigma}} \, = \, \frac{1}{n - K} \, \sum_{\ell = 1}^K
(n_\ell - 1) \widehat{\mathbf{\Sigma}}_\ell \, ,
$$
where 
$$
\widehat{\mathbf{\Sigma}}_\ell \, = \, 
\frac{1}{n_\ell - 1} \, \sum_{j \in {\cal N}_\ell} 
\left( \mathbf{x}_j - \bar{\mathbf{x}}_\ell \right) 
\left( \mathbf{x}_j - \bar{\mathbf{x}}_\ell \right)^\top \, .
$$
and 
$$
\bar{\mathbf{x}}_\ell \, = \, \frac{1}{n_\ell} \sum_{i \in {\cal N}_\ell} 
\mathbf{x}_i \, .
$$

### A numerical example

Consider the example used in class, with 2 classes and a 2-dimensional 
vector of explanatory variables. We first load the data
```{r lda0}
data(vaso, package='robustbase')
```

We will compute an approximation to the optimal predictor based on the
Gaussian model behind LDA (assuming a Gaussian distribution for the vector
of features in each class, with a common covariance matrix). 
The goal is to classify a new observation at $\mathbf{x}_0 = (\mbox{Volume}, \mbox{Rate})^\top = (1.9, 1.3)^\top$. 

For this, we first estimate the parameters of the distributions of 
$\mathbf{X} | G = 0$ and $\mathbf{X} | G = 1$. These are the two vectors of 
means:
$\mathbf{\mu}_0 \in \mathbb{R}^2$, $\mathbf{\mu}_1 \in \mathbb{R}^2$, and the common 
covariance matrix $\mathbf{\Sigma} \in \mathbb{R}^{2 \times 2}$. 
Since we will have to perform a couple of operations on each subgroup (compute
their sample means and covariance matrices), we first create two new objects, one
for the training data of each class. This is not necessary, but it results in shorter and easier to read
code. 
We call each subgrup `d0` (for class "0") and `d1` (for class "1"):
```{r lda00}
d0 <- subset(vaso, Y == 0, select = c('Volume', 'Rate')) 
d1 <- subset(vaso, Y == 1, select = c('Volume', 'Rate')) 
```
The vectors of
means for each class are
```{r lda01}
( mu0 <- colMeans( d0 ) )
( mu1 <- colMeans( d1 ) )
```
To estimate $\mathbf{\Sigma}$ we first compute the covariance matrix of each 
group:
```{r lda02}
( sigma0 <- cov( d0 ) )
( sigma1 <- cov( d1 ) )
```
and then use their weighted average, using the number of observations in each
group:
```{r lda03}
(sigma.hat <- ( (nrow(d0) - 1 ) * sigma0 
               + (nrow(d1) - 1) * sigma1 ) / (nrow(vaso) - 2) )
```
We now estimate the probabilities $P( G = 0 )$ and $P( G = 1 )$:
```{r lda04}
( pi0 <- with(vaso, mean(Y == 0)) )
( pi1 <- with(vaso, mean(Y == 1)) )
```
We are now ready to compute estimates of
$f(\mathbf{x}_0 \, | \, G = 0) P( G = 0)$ and 
$f(\mathbf{x}_0 \, | \, G = 1) P( G = 1)$. We do it first by hand, then
using pre-coded multivariate Gaussian densities in `R` and finally compare
the results using the function `lda` in package `MASS`, as done in the
lecture notes. 

#### By hand

To do it by hand we use the formula of the density of a multivariate normal 
distribution, as above. We first compute the exponents in the exponential
function `cp0` and `cp1` below, and then the value of the density function:

```{r lda1}
x0 <- c(1.9, 1.3)
p <- 2
cp0 <- as.numeric( crossprod( x0 - mu0, solve(sigma.hat, x0 - mu0)) )
dens0 <- det(sigma.hat)^(-1/2) * exp( - cp0 / 2 ) * (sqrt(2*pi)^(-p))
cp1 <- as.numeric( crossprod( x0 - mu1, solve(sigma.hat, x0 - mu1)) )
dens1 <- det(sigma.hat)^(-1/2) * exp( - cp1 / 2 ) * (sqrt(2*pi)^(-p))
```
(Note that we could have just used `exp( - cp0 / 2 )` and `exp( - cp1 / 2 )` 
above, since the other terms are the same for both expressions). 
We now compute
$f(\mathbf{x}_0 \, | \, G = 0) P( G = 0)$:
```{r lda2}
( pp0 <- dens0 * pi0 )
```
and 
$f(\mathbf{x}_0 \, | \, G = 1) P( G = 1)$:
```{r lda3}
( pp1 <- dens1 * pi1 )
```
We see that the (estimated) optimal classifier would classify a point
at $\mathbf{x}_0 = (\mbox{Volume}, \mbox{Rate})^\top = (1.3, 1.9)^\top$
as belonging to class 1. The corresponding conditional probabilities
for each class are:
```{r lda4}
c( posterior0 = pp0 / (pp0 + pp1), posterior1 = pp1 / (pp0 + pp1) )
```

#### Using a built-in function to compute multivariate Gaussian densities

Using the estimated parameters 
$\mathbf{\mu}_0 \in \mathbb{R}^2$, $\mathbf{\mu}_1 \in \mathbb{R}^2$, and the common 
covariance matrix $\mathbf{\Sigma} \in \mathbb{R}^{2 \times 2}$ as before, 
we can compute the corresponding values of Gaussian density functions using 
the function `dmvnorm` in package `mvtnorm` (see its help page for more
details):
```{r lda5}
dens0 <- mvtnorm::dmvnorm(x0, mean=mu0, sigma=sigma.hat)
dens1 <- mvtnorm::dmvnorm(x0, mean=mu1, sigma=sigma.hat)
```
And now, the numerators of the conditional probabilities and the 
conditional probabilities are:
```{r lda6}
( pp0 <- dens0 * pi0 )
( pp1 <- dens1 * pi1 )
c( posterior0 = pp0 / (pp0 + pp1), posterior1 = pp1 / (pp0 + pp1) )
```

#### Using `MASS::lda`

We now compare the above results with 
the output of the function `lda` in package `MASS` as in the lecture slides and notes
(pay attention to the effort needed to transform the object `x0` into 
an appropriate data frame to use with `predict.lda`): 
```{r lda.mass}
library(MASS)
a.lda <- lda(Y ~ Volume + Rate, data=vaso)
names(x0) <- c('Volume', 'Rate')
predict(a.lda, newdata=data.frame(t(x0)))$posterior
```
